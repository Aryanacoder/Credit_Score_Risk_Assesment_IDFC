# -*- coding: utf-8 -*-
"""PanIIT Solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QhD94iuYWXYbLWWV3u41y_eAz99QkhHo
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 1. Importing All Libraries"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, log_loss, roc_curve, auc
from sklearn.metrics import roc_auc_score, roc_curve, classification_report, precision_score, recall_score, f1_score

"""# 2. Loading the Dataset"""

# Provide the path to the dataset on your Google Drive
file_path = '/content/drive/MyDrive/Colab Notebooks/Dev_data_to_be_shared.csv'

data = pd.read_csv(file_path)

data.head()

data['bad_flag']

data['bad_flag'].value_counts()       # Data is Highly imbalance

data.shape

data.info()              # all attributes have int and float value , none of them have object dtype so no problem

data['bad_flag'].isnull().sum()             # Checking For missing value in Target Variable

data['account_number'].duplicated().sum()   # Checking for duplicate data

# Checking For null values in other Attributes

data.isnull().sum()

data.shape

"""# 3. Removing Columns with Excessive Missing Data"""

# Function to remove attributes with more than 60% missing values
def remove_high_missing_columns(df, threshold=0.6):
    missing_ratio = df.isnull().mean()
    columns_to_drop = missing_ratio[missing_ratio > threshold].index
    print(f"Removing columns with more than {threshold*100}% missing values: {list(columns_to_drop)}")
    return df.drop(columns=columns_to_drop)

# Apply the function to remove columns with more than 60% missing values
data = remove_high_missing_columns(data)

data.shape       # 1216-1199 column removed

"""#  4. Replace missing values with the Median"""

# Function to replace missing values with the median
def replace_missing_with_median(df):
    print("Replacing missing values with median...")
    return df.fillna(df.median())

# Apply the function to replace missing values with median
data = replace_missing_with_median(data)

data.head()

"""# 5. Train-Test Split"""

# Separate target variable and features
target = 'bad_flag'
features = [col for col in data.columns if col != target and col != 'account_number']

# Split data into train and test sets
X = data[features]
y = data[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

X_train.shape

X_test.shape

y.value_counts()       # Data is Highly imbalance

y.shape

"""# 6. SMOTE to oversample the minority class"""

from imblearn.over_sampling import SMOTE

# Apply SMOTE to oversample the minority class
smote = SMOTE(random_state=42)
X_train_smote_1, y_train_smote = smote.fit_resample(X_train, y_train)

# Shift data to make it non-negative
min_values = X_train_smote_1.min(axis=0)  # Find minimum values in each column
X_train_smote = X_train_smote_1 + abs(min_values) if any(min_values < 0) else X_train_smote_1

X_train_smote.shape

y_train_smote.value_counts()

y_train_smote.shape

"""# 7. Standardization"""

# Standardize the selected features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_smote)
X_test_scaled = scaler.transform(X_test)

X_train_scaled.shape

X_test_scaled.shape

"""# 8. Feature selection using PCA"""

from sklearn.decomposition import PCA

pca = PCA(n_components=50)  # Retain top 50 principal components
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

X_train_pca.shape

X_test_pca.shape

"""# 9. Loading Of validation dataset"""

val_file_path = '/content/drive/MyDrive/Colab Notebooks/validation_data_to_be_shared.csv'

val_data = pd.read_csv(val_file_path)

val_data.head()

val_data.shape

# Droping the same columns in the validation dataset which have missing data more tha 60%
val_data.drop(columns=['bureau_148', 'bureau_433', 'bureau_435', 'bureau_436', 'bureau_437', 'bureau_438', 'bureau_444', 'bureau_446', 'bureau_447', 'bureau_448', 'bureau_449', 'onus_attribute_43', 'onus_attribute_44', 'onus_attribute_45', 'onus_attribute_46', 'onus_attribute_47', 'onus_attribute_48'], inplace=True)

val_data.shape

val_data.fillna(data.median(), inplace=True)  # Replace missing values with training data median

# Preprocess validation dataset
val_data_scaled = scaler.transform(val_data[features])  # Standardize

val_data_pca = pca.transform(val_data_scaled)  # Apply PCA

"""# 10. Building the Models: Logistic Regression"""

# Train and evaluate Logistic Regression
lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train_pca, y_train_smote)
lr_y_pred = lr_model.predict(X_test_pca)
lr_y_pred_proba = lr_model.predict_proba(X_test_pca)[:, 1]
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_y_pred_proba)
lr_auc = roc_auc_score(y_test, lr_y_pred_proba)
print(f"Logistic Regression AUC-ROC: {lr_auc:.4f}")
print("Logistic Regression Classification Report:\n", classification_report(y_test, lr_y_pred))

# Plot ROC Curves
plt.figure(figsize=(10, 8))
plt.plot(lr_fpr, lr_tpr, label=f'Logistic Regression (AUC = {lr_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend(loc='lower right')
plt.grid(alpha=0.4)
plt.show()

lr_y_pred[0:10]

lr_y_pred_proba[0:10]

"""# 11. Prediction on Validation Dataset

"""

lr_y_pred_val = lr_model.predict(val_data_pca)

lr_y_pred_proba_val = lr_model.predict_proba(val_data_pca)[:, 1]

lr_y_pred_val[0:10]

lr_y_pred_proba_val[0:10]

"""# 12. Creating CSV FIle"""

# Create a DataFrame with account_number and the predicted probability
result_df = pd.DataFrame({
    'account_number': val_data['account_number'],
    'Predicted Probability': lr_y_pred_proba_val
})

result_df.head()

# Save the DataFrame to a CSV file
result_df.to_csv('predicted_probabilities.csv', index=False)
print("CSV file with predicted probabilities has been saved.")

